---
title: 机器学习算法之聚类算法
date: 2020-02-24 18:34:27
tags:
- 算法
- 机器学习
categories:
- 算法
- 机器学习
---

<center>Editor：闫玉良</center>

今日开启新算法的学习「聚类算法」

<!--more-->

***更多精彩文章请关注公众号『Pythonnote』或者『全栈技术精选』***

## 1.认识聚类算法

### 1.1 应用

1) 用户画像，广告推荐，`Data Segmentation`，搜索引擎的流量推荐，恶意流量识别

2) 基于位置信息的商业推送，新闻聚类，筛选排序

3) 图像分割，降维，识别；离群点检测；信用卡异常消费；发掘相同功能的基因片段

### 1.2 概念

**聚类算法**：

一种典型的**无监督**学习算法，主要用于将相似的样本自动归到一个类别中。

在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。

### 1.3 与分类算法最大的区别

聚类算法是无监督的学习算法，而分类算法属于监督的学习算法。

## 2.聚类算法 API 初步使用

### 2.1 API 介绍

```python
sklearn.cluster.KMeans(n_clusters=8)
"""
参数:
  n_clusters:开始的聚类中心数量
    - 整型，缺省值=8，生成的聚类数，即产生的质心（centroids）数。
    
方法:
  estimator.fit(x)
  estimator.predict(x)
  estimator.fit_predict(x)
    - 计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(x),然后再调用predict(x)
"""
```

### 2.2 案例

随机创建不同二维数据集作为训练集，并结合 `k-means` 算法将其聚类，你可以尝试分别聚类不同数量的簇，并观察聚类效果：

![image-20190219163451509](https://s2.ax1x.com/2020/02/24/33LfPI.png)

聚类参数 `n_cluster` 传值不同，得到的聚类结果不同

![image-20190219163505530](https://s2.ax1x.com/2020/02/24/33LXin.png)

#### 2.2.1 流程分析

![image-20190219163649472](https://s2.ax1x.com/2020/02/24/33OpsU.png)

#### 2.2.2 代码实现

1) 创建数据集

```python
import matplotlib.pyplot as plt
from sklearn.datasets.samples_generator import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import calinski_harabaz_score

# 创建数据集
# X为样本特征，Y为样本簇类别， 共1000个样本，每个样本4个特征，共4个簇，
# 簇中心在[-1,-1], [0,0],[1,1], [2,2]， 簇方差分别为[0.4, 0.2, 0.2, 0.2]
X, y = make_blobs(n_samples=1000, n_features=2, centers=[[-1, -1], [0, 0], [1, 1], [2, 2]],
                  cluster_std=[0.4, 0.2, 0.2, 0.2],
                  random_state=9)

# 数据集可视化
plt.scatter(X[:, 0], X[:, 1], marker='o')
plt.show()
```

2) 使用 `k-means` 进行聚类,并使用 `CH` 方法评估

``` python
y_pred = KMeans(n_clusters=2, random_state=9).fit_predict(X)
# 分别尝试n_cluses=2\3\4,然后查看聚类效果
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()

# 用Calinski-Harabasz Index评估的聚类分数
print(calinski_harabaz_score(X, y_pred))
```

## 3.聚类算法实现流程

**`k-means` 其实包含两层内容：**

`K` ：初始中心点个数（计划聚类数）

`means`：求中心点到其他数据点距离的平均值

### 3.1 k-means 聚类步骤

1) 随机设置 `K` 个特征空间内的点作为初始的聚类中心

2) 对于其他每个点计算到 `K` 个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别

3) 紧接着，重新计算出每个聚类的新中心点（平均值）

4) 如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程

通过下图解释实现流程：

![K-meansè¿‡ç¨‹åˆ†æž](https://gitee.com/Ethanyan/pic_data/raw/master/K-means%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90.png)

k聚类动态效果图

![2019-02-19 17.06.49](https://gitee.com/Ethanyan/pic_data/raw/master/2019-02-19%2017.06.49.gif)

### 3.2 案例练习

案例：

![image-20190219171158984](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219171158984.png)

1) 随机设置 `K` 个特征空间内的点作为初始的聚类中心（本案例中设置 `p1` 和 `p2` ）

![image-20190219171244828](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219171244828.png)

2) 对于其他每个点计算到 `K` 个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别

![image-20190219171326923](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219171326923.png)

![image-20190219171338441](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219171338441.png)

3) 重新计算出每个聚类的新中心点（平均值）

![image-20190219171727035](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219171727035.png)

4) 如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程「经过判断，需要重复上述步骤，开始新一轮迭代」

![image-20190219171951607](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219171951607.png)

![image-20190219172011618](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219172011618.png)

5) 当每次迭代结果不变时，认为算法收敛，聚类完成，**`K-Means` 一定会停下，不可能陷入一直选质心的过程。**

![image-20190219172125388](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219172125388.png)

### 3.3 小结

**流程**：

1) 事先**确定常数 `K`**，常数 `K` 意味着最终的聚类类别数;

2) 首先随机**选定初始点为质心**，并通过计算每一个样本与质心之间的相似度(这里为欧式距离)，将样本点归到最相似的类中，

3) 接着，**重新计算**每个类的质心(即为类中心)，重复这样的过程，直到**质心不再改变**，

4) 最终就确定了每个样本所属的类别以及每个类的质心。

**注意**：由于每次都要计算所有的样本与每一个质心之间的相似度，故在大规模的数据集上，`K-Means` 算法的收敛速度比较慢。

## 4.模型评估

### 4.1 误差平方和(SSE \The sum of squares due to error)

举例：(下图中数据-0.2, 0.4, -0.8, 1.3, -0.7, 均为真实值和预测值的差)

![image-20190308211436382](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190308211436382.png)

在 `k-means` 中的应用：

![image-20190219173503991.png](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219173503991.png)

![image-20190219173610490](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219173610490.png)

公式各部分内容:

![image-20190308211313308](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190308211313308.png)

上图中: `k=2`

**1) `SSE` 图最终的结果，对图松散度的衡量。**(eg: `SSE(左图)` < `SSE(右图)`)

2) `SSE` 随着聚类迭代，其值会越来越小，直到最后趋于稳定

3) 如果质心的初始值选择不好，`SSE` 只会达到一个不怎么好的局部最优解

### 4.2「肘」方法(Elbow method) — K值确定

1) 对于 `n` 个点的数据集，迭代计算 `k from 1 to n`，每次聚类完成后计算每个点到其所属的簇中心的距离的平方和；

2) 平方和是会逐渐变小的，直到 `k==n` 时平方和为0，因为每个点都是它所在的簇中心本身。

3) 在这个平方和变化过程中，会出现一个拐点也即「肘」点，**下降率突然变缓时即认为是最佳的 `k` 值**。

在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在**增加分类无法带来更多回报时，我们停止增加类别**。

### 4.3 轮廓系数法(Silhouette Coefficient)

结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果：

![image-20190219174853018](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219174853018.png)

**目的：**内部距离最小化，外部距离最大化

![image-20190219175813875](https://gitee.com/Ethanyan/pic_data/raw/master/image-20190219175813875.png)

计算样本 `i` 到同簇其他样本的平均距离 `ai`，`ai` 越小样本 `i` 的簇内不相似度越小，说明样本 `i` 越应该被聚类到该簇。

计算样本 `i` 到最近簇 `Cj` 的所有样本的平均距离 `bij`，称样本 `i` 与最近簇 `Cj` 的不相似度，定义为样本 `i` 的簇间不相似度：`bi =min{bi1, bi2, ..., bik}`，`bi` 越大，说明样本 `i` 越不属于其他簇。

求出所有样本的轮廓系数后再求平均值就得到了**平均轮廓系数**。

平均轮廓系数的取值范围为[-1,1]，系数越大，聚类效果越好。

簇内样本的距离越近，簇间样本距离越远。

### 4.4 CH 系数(Calinski-Harabasz Index)

**Calinski-Harabasz：**

类别内部数据的协方差越小越好，类别之间的协方差越大越好（换句话说：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好），这样的 `Calinski-Harabasz` 分数 `s` 会高，分数 `s` 高则聚类效果越好。

![image-20190219182033877](https://s2.ax1x.com/2020/02/24/38u9OO.png)

`tr` 为**矩阵的迹**, `Bk` 为类别之间的协方差矩阵，`Wk` 为类别内部数据的协方差矩阵;

`m` 为训练集样本数，`k` 为类别数。

![image-20190219182615777](https://s2.ax1x.com/2020/02/24/38uept.png)

使用矩阵的迹进行求解的理解：

矩阵的对角线可以表示一个物体的相似性

在机器学习里，主要为了获取数据的特征值，那么就是说，在任何一个矩阵计算出来之后，都可以简单化，只要获取矩阵的迹，就可以表示这一块数据的最重要的特征了，这样就可以把很多无关紧要的数据删除掉，达到简化数据，提高处理速度。

`CH` 需要达到的目的：**用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。**

### 4.5 总结

1) **肘部法**：下降率突然变缓时即认为是最佳的 `k` 值

2) **`SC` 系数**：取值为[-1, 1]，其值越大越好

3)  **`CH`系数**：分数 `s` 高则聚类效果越好

## 5.算法优化

### 5.1 k-means 算法小结

**优点：**

 1) 原理简单（靠近中心点），实现容易

 2) 聚类效果中上（依赖K的选择）

 3) 空间复杂度 `O(N)`，时间复杂度 `O(IKN)`

> `N` 为样本点个数，`K` 为中心点个数，`I` 为迭代次数

**缺点：**

 1) 对离群点，噪声敏感 （中心点易偏移）

 2) 很难发现大小差别很大的簇及进行增量计算

 3) 结果不一定是全局最优，只能保证局部最优（与 `K` 的个数及初值选取有关）

### 5.2 Canopy 算法配合初始聚类

#### 5.2.1 实现流程

![image-20190219190832599](https://s2.ax1x.com/2020/02/24/38KJVe.png)

#### 5.2.2 优缺点

**优点：**

 1) `Kmeans` 对噪声抗干扰较弱，通过 `Canopy` 对比，将较小的 `NumPoint` 的 `Cluster` 直接去掉有利于抗干扰。

 2) `Canopy` 选择出来的每个 `Canopy` 的 `centerPoint` 作为 `K` 会更精确。

 3) 只是针对每个 `Canopy` 的内做 `Kmeans` 聚类，减少相似计算的数量。

**缺点：**

 1) 算法中 `T1`、`T2` 的确定问题 ，依旧可能落入局部最优解

### 5.3 K-means++

![image-20190219233830941](https://s2.ax1x.com/2020/02/24/38KvM6.png)

![image-20190219233845360](https://s2.ax1x.com/2020/02/24/38MCIH.png)

![image-20190219233904862](https://s2.ax1x.com/2020/02/24/38MkRI.png)

![image-20190219233921494](https://s2.ax1x.com/2020/02/24/38Mudg.png)

`kmeans++` 目的，让选择的质心尽可能的分散

如下图中，如果第一个质心选择在圆心，那么最优可能选择到的下一个点在 `P(A)` 这个区域（根据颜色进行划分）

![image-20190219234135506](https://s2.ax1x.com/2020/02/24/38M1Wn.png)

### 5.4 二分 k-means

**实现流程：**

1) 所有点作为一个簇

2) 将该簇一分为二

3) 选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。

4) 以此进行下去，直到簇的数目等于用户给定的数目 `k`为止。

![image-20190323000108301](https://s2.ax1x.com/2020/02/24/38M6OK.png)

**隐含的一个原则**

因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于他们的质心，聚类效果就越好。所以需要对误差平方和最大的簇进行再一次划分，因为误差平方和越大，表示该簇聚类效果越不好，越有可能是多个簇被当成了一个簇，所以我们首先需要对这个簇进行划分。

二分 `K` 均值算法可以加速 `K-means` 算法的执行速度，因为它的相似度计算少了并且不受初始化问题的影响，因为这里不存在随机点的选取，且每一步都保证了误差最小。

### 5.5 k-medoids（k-中心聚类算法）

`K-medoids` 和 `K-means` 是有区别的，**不一样的地方在于中心点的选取**

> `K-means` 中，将中心点取为当前 `cluster` 中所有数据点的平均值，对异常点很敏感!
>
> `K-medoids` 中，将从当前 `cluster` 中选取到其他所有（当前 `cluster` 中的）点的距离之和最小的点作为中心点。

![image-20190220000002208](https://s2.ax1x.com/2020/02/24/38M7Of.png)

**算法流程：**

1) 总体 `n` 个样本点中任意选取 `k` 个点作为 `medoids`

2) 按照与 `medoids` 最近的原则，将剩余的 `n-k` 个点分配到当前最佳的 `medoids` 代表的类中

3) 对于第 `i` 个类中除对应 `medoids` 点外的所有其他点，按顺序计算当其为新的 `medoids` 时，代价函数的值，遍历所有可能，选取代价函数最小时对应的点作为新的 `medoids`

4) 重复2-3的过程，直到所有的 `medoids` 点不再发生变化或已达到设定的最大迭代次数

5) 产出最终确定的 `k` 个类

**k-medoids对噪声鲁棒性好。**

例：当一个 `cluster` 样本点只有少数几个，如（1,1）（1,2）（2,1）（1000,1000）。其中（1000,1000）是噪声。如果按照 `k-means` 质心大致会处在（1,1）（1000,1000）中间，这显然不是我们想要的。这时 `k-medoids` 就可以避免这种情况，他会在（1,1）（1,2）（2,1）（1000,1000）中选出一个样本点使 `cluster` 的绝对误差最小，计算可知一定会在前三个点中选取。

`k-medoids` 只能对小样本起作用，样本大，速度就太慢了，当样本多的时候，少数几个噪音对 `k-means` 的质心影响也没有想象中的那么重，所以 `k-means` 的应用明显比 `k-medoids` 多。

### 5.6 Kernel k-means（了解）

`kernel k-means` 实际上，就是将每个样本进行一个投射到高维空间的处理，然后再将处理后的数据使用普通的 `k-means` 算法思想进行聚类。

![image-20190219235240810](https://s2.ax1x.com/2020/02/24/38Qm11.png)

### 5.7 ISODATA（了解）

类别数目随着聚类过程而变化；

对类别数会进行合并，分裂；

「合并」当聚类结果某一类中样本数太少，或两个类间的距离太近时

「分裂」当聚类结果中某一类的类内方差太大，将该类进行分裂

### 5.8 Mini Batch K-Means（了解）

适合大数据的聚类算法

大数据量是什么量级？通常当样本量大于1万做聚类时，就需要考虑选用`Mini Batch K-Means` 算法。

`Mini Batch KMeans` 使用了 `Mini Batch`（分批处理）的方法对数据点之间的距离进行计算。

`Mini Batch` 计算过程中不必使用所有的数据样本，而是从不同类别的样本中抽取一部分样本来代表各自类型进行计算。由于计算样本量少，所以会相应的减少运行时间，但另一方面抽样也必然会带来准确度的下降。

该算法的迭代步骤有两步：

1) 从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心

2) 更新质心

与 `Kmeans` 相比，数据的更新在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。

### 5.9 总结

| **优化方法**       | **思路**                     |
| ------------------ | ---------------------------- |
| Canopy+kmeans      | Canopy粗聚类配合kmeans       |
| kmeans++           | 距离越远越容易成为新的质心   |
| 二分k-means        | 拆除SSE最大的簇              |
| k-medoids          | 和kmeans选取中心点的方式不同 |
| kernel kmeans      | 映射到高维空间               |
| ISODATA            | 动态聚类                     |
| Mini-batch K-Means | 大数据集分批聚类             |

***更多精彩文章请关注公众号『Pythonnote』或者『全栈技术精选』***

