---
title: 机器学习算法之逻辑回归
date: 2020-02-21 18:19:53
tags:
- 算法
- 机器学习
categories:
- 算法
- 机器学习
---

<center>Editor：闫玉良</center>

逻辑回归（`Logistic Regression`）是机器学习中的一种**分类模型**。虽然名字中带有「回归」，但它却不是回归算法，而是一种分类算法。由于此算法的简单和高效，在实际应用中使用非常广泛。也许有人很好奇，那为什么叫做「回归」而不是「分类」呢？

<!--more-->

**答：**名字中之所以包含「回归」，是因为其 `实质` ：是在线性回归的基础上，套用了一个逻辑函数。

***更多精彩文章请关注公众号『Pythonnote』或者『全栈技术精选』***

# 逻辑回归

## 1.应用场景

1) 广告点击率

> 点击率为点击者占整体浏览者的数量。整体浏览者分为点击者和非点击者。

2) 验证垃圾邮件

> 可判定邮件是垃圾邮件或者不是垃圾邮件。

3) 患病

> 是否患病。

4) 金融诈骗

> 判断此笔交易是否属于金融诈骗。

5) 虚假账号

> 是真实账号，还是虚假账号？

通过以上示例，可以发现其特点，那就是**同属于两个类别之间的判断**。逻辑回归可谓是解决二分类问题的利器。

## 2.原理

要想熟悉逻辑回归，必须掌握以下两点：

1) 逻辑回归中，其输入值是什么？

2) 如何判断逻辑回归的输出？

### 2.1 输入

![逻辑回归输入.png](https://s2.ax1x.com/2020/02/21/3nheuF.png)

逻辑回归的输入其实就是一个线性回归的结果。（正解决了大家对文章开始部分，介绍逻辑回归实质时那句话的疑问）

### 2.2 激活函数

1) `sigmoid` 函数

![sigmoid公式.png](https://s2.ax1x.com/2020/02/21/3n4ZGt.png)

2) 判断标准：回归的结果输入到 `sigmoid` 函数当中，输出结果为 [0, 1] 区间中的一个概率值，默认为0.5为阈值

![sigmoid图像.png](https://s2.ax1x.com/2020/02/21/3n40Z4.png)

> 逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于这个类别，并且这个类别默认标记为1(正例)，另外的一个类别会标记为0(反例)。（方便损失计算）

**输出结果解释：**假设有两个类别 `A` 和 `B`，并且概率值为属于 `A(1)` 类别的概率值。现在有一个样本输入到逻辑回归，输出结果为0.6，这个概率值超过0.5，意味着训练或者预测的结果就是 `A(1)` 类别。反之，如果得出结果为0.3，那么训练或者预测结果就为 `B(0)` 类别。

![逻辑回归运算过程.png](https://s2.ax1x.com/2020/02/21/3nIaE4.png)

当预测结果不准确时，在线性回归中使用了均方误差衡量损失，那么对于逻辑回归，该如何去衡量此损失呢？

## 3.损失及优化

### 3.1 损失

逻辑回归的损失，称之为**对数似然损失**，公式如下：

1) 分开类别：

![单个对数似然损失.png](https://s2.ax1x.com/2020/02/21/3nIoxP.png)

怎么理解单个的式子呢？此时就需要用到 `log` 的函数图像了：

![image-20190221142055367](https://s2.ax1x.com/2020/02/21/3noka4.png)

2) 综合完整损失函数

![完整对数似然损失.png](https://s2.ax1x.com/2020/02/21/3noDoQ.png)

> 看到这个式子，其实同我们认识的信息熵类似。
>
> 信息熵这个词是 C.E.Shannon（香农）从热力学中借用过来的。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度，把信息中排除了冗余后的平均信息量称为「信息熵」

接下来带入激活函数位置处的例子来计算一遍，就能理解其意义了。

![损失计算过程.png](https://s2.ax1x.com/2020/02/21/3noLy6.png)

> 我们已经知道，`log(P)` 中 `P` 值越大，结果越小，所以可以对着这个损失的式子去分析

### 3.2 优化

同样使用梯度下降优化算法，去减少损失函数的值。通过更新逻辑回归前面对应算法的权重参数，**提升原本属于1类别的概率，降低原本是0类别的概率。**

## 4.分类评估方法

### 4.1 混淆矩阵

在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类)

![image-20190321103913068](https://s2.ax1x.com/2020/02/21/3n7SBT.png)

### 4.2 精确率(Precision)与召回率(Recall)

1) **精确率：**预测结果为正例样本中真实为正例的比例（了解）

![image-20190321103930761](https://s2.ax1x.com/2020/02/21/3n7svq.png)

2) **召回率：**真实结果为正例的样本中预测结果为正例的比例（查得全，对正样本的区分能力）

![image-20190321103947092](https://s2.ax1x.com/2020/02/21/3n7f54.png)

***更多精彩文章请关注公众号『Pythonnote』或者『全栈技术精选』***

